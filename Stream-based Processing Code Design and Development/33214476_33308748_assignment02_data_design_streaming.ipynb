{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d28accc5",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\"> FIT3182: Big Data Management and Processing (2025) </span>\n",
    "---\n",
    "\n",
    "Teaching Team:\n",
    "\n",
    "Faculty of Information Technology, Monash University, Australia\n",
    "* A/Prof. David Taniar (Chief Examiner) | david.taniar@monash.edu\n",
    "\n",
    "School of Information Technology, Monash University, Malaysia\n",
    "* Vishnu Monn (Unit Coordinator) | vishnu.monn@monash.edu\n",
    "* Shageenderan Sapai | shageenderan.sapai@monash.edu\n",
    "* Henry Quan Bi Pay | quan.pay@monash.edu\n",
    "* Ruturaj Reddy | ruturaj.reddy@monash.edu\n",
    "* Chai Wai Jin (Class Assistant) | wcha0106@student.monash.edu\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af82432",
   "metadata": {},
   "source": [
    "# <span style=\"color:#0b486b\">  Group Information</span>\n",
    "---\n",
    "Note: Group members need to be enrolled in the same tutorial day and time slot.\n",
    "\n",
    "Your tutorial day and time: **Friday 4-6pm**    <br/>\n",
    "\n",
    "1st group member\n",
    "\n",
    "Surname: **Yow**  <br/>\n",
    "Firstname: **Zoe**    <br/>\n",
    "Student ID: **33214476**    <br/>\n",
    "Email: **zyow0001@student.monash.edu**    <br/>\n",
    "\n",
    "2nd group member\n",
    "\n",
    "Surname: **Phua**  <br/>\n",
    "Firstname: **Yee Yen**    <br/>\n",
    "Student ID: **33308748**    <br/>\n",
    "Email: **yphu0007@student.monash.edu**    <br/>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10027af0",
   "metadata": {},
   "source": [
    "# Streaming Application\n",
    "### Due: <span style=\"color:red\">11:55pm MYT, 27th May 2025</span>  (Tuesday)\n",
    "\n",
    "#### <span style=\"color:red\">Important note:</span> This is an **group** assignment with two students (max) per group. You or your group partner can share the code and outcomes of this assignment. However, you should not attempt to post questions on EdForum or any other online platform seeking solutions to the answers. If you require clarification on the assignment questions, you can post a post on EdForum or seek consultation from the tutors. In addition, AI and generative tools may be used in Guided ways.  However, students will be required to demonstrate a comprehensive understanding of the submitted work, failing which significant marks will be deducted from the submitted work. Even though this is a group work, each student is required to submit the assignment work in Moodle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cb43d2",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "This notebook has been prepared for you to complete Assignment 2. The theme of this assignment is about practical knowledge and skills in streaming application using Spark and Kafka. **The total marks for this notebook is 30 marks, which is equivalent to 30 percentage points of the total coursework marks for this unit.**\n",
    "\n",
    "* Before getting started, you should read the entire notebook carefully once to understand what you need to do.\n",
    "\n",
    "* Always use the data from the provided `.csv` files to answer the questions unless stated otherwise.\n",
    "\n",
    "This assignment contain **3 parts**:\n",
    "\n",
    "* **Part 1**: MongoDB Data Model (5 Marks)\n",
    "* **Part 2**: Streaming Application (20 Marks)\n",
    "* **Part 3**: Documentation and comments to describe the proposed solution in the submitted notebook (5 Marks)\n",
    "* **Part 4**: Code demo and interview (Negative marking)\n",
    "\n",
    "Required Software:\n",
    "\n",
    "* You will be using Python 3. Answer all questions inside this Jupyter Notebook\n",
    "* Please use the provided Docker to load the Jupyter Notebook\n",
    "\n",
    "**Hint**: This assignment was essentially designed based on the seminars and applied sessions covered from Week 6 to Week 11. You are strongly encouraged to go through these contents thoroughly which might help you to complete the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d6830c",
   "metadata": {},
   "source": [
    "### Assignment Marking\n",
    "\n",
    "The marking of this assignment is based on quality of work you have submitted rather than just quantity. Marking starts from 0 and goes up based on tasks you have successfully completed and their quality, for example, how well the code submitted follows programming standards, code documentation, presentation of the assignment, readability of the code, organization of the code and so on. Please find the PEP 8 -- Style Guide for Python Code [here](https://www.python.org/dev/peps/pep-0008/) for your reference. Please refer to marking guidelines in Moodle for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9652a7b4",
   "metadata": {},
   "source": [
    "### What to Submit\n",
    "\n",
    "This assignment is to be completed individually and submitted to Moodle unit site. By the due date, you are required to submit the following files to the corresponding Assignment (Dropbox) in Moodle.\n",
    "\n",
    "* **xxx_assignment02_data_design_streaming.ipynb**: this is your main Python notebook solution source file (the data design and streaming application).\n",
    "* **xxx_assignment02_producer_a/b/c.ipynb**: this is your Python notebook solution to run the Kafka producer that reads from one of the camera event files. If you are running multiple producers concurrently in the main notebook, then this file is optional.\n",
    "* **xxx_assignment02_visualisation.ipynb**: this is your Python notebook solution containing the data visualisation.\n",
    "* **xxx_assignment02_code.zip** (if applicable): this is a zip file that contains python files with custom-defined classes and functions to be used in notebook.\n",
    "\n",
    "where `xxx` represents the student ID of each group member. For example, if your student ID is <span style=\"color:red\">12345</span> and your group partner's ID is is <span style=\"color:red\">54321</span>, then your submission file name would be <span style=\"color:red\">12345_54321_assignment02_data_design_streaming.ipynb</span>. Please do the same for all of the submission files.\n",
    "\n",
    "Your assignment will be assessed based on the content of the submitted files in Moodle. We will use the same docker image as provided in this unit when marking your assignment. **If you used additional libraries, please include pip commands in your Jupyter notebook.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37f836d",
   "metadata": {},
   "source": [
    "### Plagiarism and Collusion\n",
    "\n",
    "Plagiarism and collusion are serious academic offenses at Monash University. Students must not share their work with any student. Students should consult policy linked [here](https://www.monash.edu/students/academic/policies/academic-integrity) for more information. See also the video linked on the Moodle page under the Assignment block.\n",
    "\n",
    "The submitted notebook files will be checked for collusion or plagiarism. Students suspected of colluding or plagiarising the assignment will be reported to the Student Conduct and Complaints Department for academic misconduct. Consequently, your grade for this unit will be withheld until the investigation is complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99713db4",
   "metadata": {},
   "source": [
    "### Generative AI usage\n",
    "\n",
    "AI & Generative AI tools may be used in GUIDED ways within this assessment / task as per the guidelines provided.\n",
    " \n",
    "In this task, AI can be used as specified for one or more parts of the assessment task as per the instructions.\n",
    "You may use AI to help you learn how to solve the assignment.\n",
    "\n",
    "Where used, AI must be used responsibly, clearly documented and appropriately acknowledged (see [Learn HQ](https://www.monash.edu/student-academic-success/build-digital-capabilities/create-online/acknowledging-the-use-of-generative-artificial-intelligence)).\n",
    " \n",
    "Any work submitted for a mark must:\n",
    "represent a sincere demonstration of your human efforts, skills and subject knowledge that you will be accountable for.\n",
    "adhere to the guidelines for AI use set for the assessment task.\n",
    "reflect the University’s commitment to academic integrity and ethical behaviour.\n",
    "Inappropriate AI use and/or AI use without acknowledgement will be considered a breach of academic integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6496b475",
   "metadata": {},
   "source": [
    "### Late submissions \n",
    "Extensions and other individual alterations to the assessment regime will only be considered using the University’s [Special Consideration Policy](https://www.monash.edu/students/admin/exams/changes/special-consideration). There is a 10% penalty per day, including weekends, for late submission. Please note that short extensions are not allowed for group submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe384a47",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Preliminary</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbe6061",
   "metadata": {},
   "source": [
    "### Scenario Background\n",
    "\n",
    "Malaysia’s road network consistently ranks among the busiest and most accident‑prone in Southeast Asia. Federal roads alone account for a significant proportion of traffic incidents, particularly during peak travel periods and festive seasons, when speed limits of up to 90 km/h (and 110 km/h on expressways) are frequently exceeded in an effort to cover long distances quickly. Since 2012, the Automated Enforcement System (AES) has deployed static speed‑light and red‑light cameras at fixed points to deter speeding and dangerous cornering. However, these point‑capture devices suffer from well‑documented loopholes: drivers can simply decelerate when approaching a camera and then accelerate immediately afterward, rendering enforcement uneven and often ineffective.\n",
    "\n",
    "To address these shortcomings, the Malaysian Government has begun rolling out the Automated Awareness Safety System (AWAS), a point‑to‑point average‑speed enforcement mechanism (Jamil et al., 2022). Figure 1 illustrates an overview of the AWAS system. AWAS leverages pairs of Ekin Spotter modular cameras equipped with 360° video surveillance and Automatic Number Plate Recognition (ANPR) to record each vehicle’s passage at two distinct checkpoints along a highway segment. By logging the exact timestamps at “Point A” and “Point B,” the system computes the travel time over a known distance (typically 1–5 km) and derives the average speed. Any average exceeding the legal limit (e.g., 110 km/h on expressways) automatically triggers a violation notice, regardless of momentary decelerations.\n",
    "\n",
    "While AWAS promises more consistent enforcement, it also introduces significant data‑processing challenges. Each camera pair generates a continuous stream of high‑volume events—potentially thousands per minute during peak hours—that must be matched by license plate, ordered by event time, and joined across streams to compute speeds in near real time. The system must tolerate out‑of‑order or late‑arriving events (e.g., network delays), bound state growth via watermarks, and guarantee end‑to‑end exactly‑once processing to prevent duplicate violation records. These requirements make AWAS an ideal case study for a streaming Big Data architecture using Apache Kafka for ingestion, Apache Spark Structured Streaming for stateful stream–stream joins, and MongoDB for scalable storage of both raw events and flagged violations.\n",
    "\n",
    "Reference:\n",
    "\n",
    "Jamil, H. M., Shabadin, A., & Ibrahim, M. K. A. (2022). Automated Awareness Safety System (AwAS) for Red Light Running in Malaysia: An Analysis of Four-year Data on Its Effectiveness. Journal of the Society of Automotive Engineers Malaysia, 6(1), 19-29."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fc8e2b",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"FIT3182_A2_Fig_1.png\"></img>\n",
    "    <p style=\"te\n",
    "    xt-align: center\">Figure 1 - Overview of AWAS</p>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9965391",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "In this assignment, you are provided the following `.csv` files to help you simulate the AWAS streaming application. The following details the information about the dataset.\n",
    "\n",
    "#### vehicle.csv\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* owner_name (a string that contains name of the owner)\n",
    "* owner_addr (a string that contains the address of the owner)\n",
    "* vechicle_type (a string that represents the vechile model)\n",
    "* registration_date (date and time when the vehicle was registered)\n",
    "\n",
    "#### camera.csv\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* latitude (a float value representing latitude of camera)\n",
    "* longitude (a float value representing longitude of camera)\n",
    "* position (a float value tells at which kilometer point is the camera)\n",
    "* speed_limit (a float value of maximum legal speed for the segment)\n",
    "\n",
    "#### camera_event.csv\n",
    "* event_id (a string-based unique identifier to camera reading)\n",
    "* batch_id (a integer-based identifier to batch reading)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id (an integer-based unique identifier to camera location)\n",
    "* timestamp (a string that tells the timestamp when the vehicle passed the camera)\n",
    "* speed_reading (a float value that tells the instantaneous speed, recorded in km/h, by that camera)\n",
    "\n",
    "#### camera_event_historic.csv\n",
    "* violation_id (a string-based unique identifier for violation record)\n",
    "* car_plate (a string-based unique identifier to each vehicle)\n",
    "* camera_id_start (an integer-based unique identifier to starting camera location)\n",
    "* camera_id_end (an integer-based unique identifier to ending camera location)\n",
    "* timestamp_start (a string that tells the timestamp when the vehicle passed the starting camera)\n",
    "* timestamp_end (a string that tells the timestamp when the vehicle passed the ending camera)\n",
    "* speed_reading (a float value that tells the average speed, recorded in km/h, within the camera segment)\n",
    "\n",
    "<span style=\"color:red\">Important note:</span> Multiple files of camera_event.csv will be provided, each corresponds to a camera respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080a9aab",
   "metadata": {},
   "source": [
    "### Required Imports\n",
    "\n",
    "Import necessary Python modules in the cell below. Include `pip` statement if external libraries/modules are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce3d069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add pip statement if necessary\n",
    "#!pip3 install sodapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cadfa767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any imports here\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "import json\n",
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka3 import KafkaProducer\n",
    "import random\n",
    "import datetime as dt\n",
    "import pymongo\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, expr, when, current_timestamp\n",
    "from pymongo import ASCENDING\n",
    "import uuid\n",
    "from datetime import timezone\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b0dee",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 1: MongoDB Data Model</span>\n",
    "\n",
    "This section consists of 3 sub-questions\n",
    "\n",
    "In this task, you will study the data model of a streaming application. You will demonstrate the theoretical knowledge by designing appropriate data model based on the provided dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a97125d",
   "metadata": {},
   "source": [
    "### Task 1.1 Collection Design\n",
    "\n",
    "In this task, design **at least** the following 3 collections. Add other collections if they are necessary.\n",
    "* Vehicle (Store static metadata about each vehicle)\n",
    "* Camera (Store static definitions of each camera)\n",
    "* Violation (Records of flagged violations)\n",
    "\n",
    "For each collection, provide\n",
    "* 1-2 sentence description of why this collection exists\n",
    "* document schema and a sample document\n",
    "* indexes (if any) by specifying\n",
    "    * Fields (and sort order if applicable)\n",
    "    * Type\n",
    "    * Purpose of the index\n",
    "* shard key strategy (if any) by specifying\n",
    "    * Chosen shard key\n",
    "    * Shard key type\n",
    "    * Rationale\n",
    "* data retention policy (if applicable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "236ae0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MongoClient('mongodb://10.192.64.67:27017/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "092fbe6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['admin', 'config', 'local']\n"
     ]
    }
   ],
   "source": [
    "result = client.list_database_names()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "654da6a9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database(MongoClient(host=['10.192.64.67:27017'], document_class=dict, tz_aware=False, connect=True), 'fit3182_db')\n"
     ]
    }
   ],
   "source": [
    "db = client.fit3182_db\n",
    "print(db)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719287ea",
   "metadata": {},
   "source": [
    "### Vehicle\n",
    "\n",
    "It holds static metadata about each registered vehicle so that any violation or lookup by car_plate can immediately retrieve owner and registration details.\n",
    "\n",
    "Below is the code that creates the Vehicle Collections and adds the CSV data into the collection, it also creates an index on \"car_plate\" in ascending order and is not a unique type since there are duplicates of car plate numbers which different owner identities. The purpose of this index is to provide a fast lookup when processing violations to retrieve vehicle metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f932a0f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'car_plate_1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(db.list_collection_names())\n",
    "vehicles = db.vehicles\n",
    "vehicles.create_index([(\"car_plate\", pymongo.ASCENDING)], unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd914590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_id_', 'car_plate_1']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(list(vehicles.index_information()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a49f7068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 10000 documents\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/student/data/vehicle.csv\")\n",
    "\n",
    "records = df.to_dict(orient='records')\n",
    "\n",
    "if records:\n",
    "    result = vehicles.insert_many(records)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cf7e88",
   "metadata": {},
   "source": [
    "#### Vehicle Collection Schema and Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1beca93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"_id\": { \"$oid\": \"650e8400e29b41d4a7164466\" },\n",
    "#   \"car_plate\": \"CJW924\",\n",
    "#   \"owner_name\": \"Alice Tan\",\n",
    "#   \"owner_addr\": \"123 Jalan Ampang, Kuala Lumpur\",\n",
    "#   \"vehicle_type\": \"Toyota Corolla 2019\",\n",
    "#   \"registration_date\": { \"$date\": \"2019-06-15T10:30:00Z\" }\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6a70d0",
   "metadata": {},
   "source": [
    "#### Vehicle Collection Schema and Sample Document Data Type\n",
    "\n",
    "_id: ObjectId (internal key)\n",
    "\n",
    "car_plate: string, unique business key\n",
    "\n",
    "owner_name / owner_addr: strings\n",
    "\n",
    "vehicle_type: string model identifier\n",
    "\n",
    "registration_date: BSON Date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9195afe",
   "metadata": {},
   "source": [
    "#### Vehicle Collection Shard Key Strategy\n",
    "\n",
    "We are not using sharding. This is because, the vehicle set is relatively small and mostly reads; a single shard easily handles lookup throughput and ensures strong consistency for updates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf26fa36",
   "metadata": {},
   "source": [
    "### Camera\n",
    "\n",
    "The Camera Collection stores each camera's fixed location and operational parameters (position and speed limit) so violations can be geo‐referenced and speed thresholds applied.\n",
    "\n",
    "Below is the code that creates the Camera Collection and adds the CSV data into the collection, it also includes creating an index for based on the camera_id in ascending order. Although this collection is small and the lookup cost would not be expensive, we find that it would be more effective to do so for future expansion of the code.(relate this back to big data and real-world scenario) The purpose of this index is for fast lookup when processing violations to retrieve camera metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45bbd506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vehicles']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'camera_id_1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(db.list_collection_names())\n",
    "cameras = db.cameras\n",
    "cameras.create_index([(\"camera_id\", pymongo.ASCENDING)], unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae379f83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 3 documents\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/student/data/camera.csv\")\n",
    "\n",
    "records = df.to_dict(orient='records')\n",
    "\n",
    "if records:\n",
    "    result = cameras.insert_many(records)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a78a3f",
   "metadata": {},
   "source": [
    "#### Camera Collection Schema and Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90b02dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"_id\": { \"$oid\": \"650e8500e29b41d4a7164467\" },\n",
    "#   \"camera_id\": 1,\n",
    "#   \"latitude\": 3.1390,\n",
    "#   \"longitude\": 101.6869,\n",
    "#   \"position\": 12.5,\n",
    "#   \"speed_limit\": 90.0\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65c6e4b",
   "metadata": {},
   "source": [
    "#### Camera Collection Schema and Sample Document Data Type\n",
    "\n",
    "camera_id: integer business key\n",
    "\n",
    "latitude / longitude: double for geo‐location\n",
    "\n",
    "position: double (km marker)\n",
    "\n",
    "speed_limit: double (km/h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8102445b",
   "metadata": {},
   "source": [
    "#### Camera Collection Shard Key Strategy\n",
    "\n",
    "We are not using sharding. This is because camera count is small and static, hence sharding would add unnecessary complexity without performance benefit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad69ed56",
   "metadata": {},
   "source": [
    "### Historic Violation Collection\n",
    "\n",
    "This collection captures every past flagged violation event. It links vehicle plates, camera pairs, timestamps, and measured speeds for real-time monitoring and windowed average‐speed calculations.\n",
    "\n",
    "Below is the code that creates the Historic Violation Collection and adds the CSV data into the collection, it also includes the creation of the index on \"violation_id\" in ascending order that is unique. The purpose of this index is to provide a fast lookup when processing violations to retrieve past violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "509f14f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['vehicles', 'cameras']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'violation_id_1'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(db.list_collection_names())\n",
    "historic_violations = db.historic_violations\n",
    "historic_violations.create_index([(\"violation_id\", pymongo.ASCENDING)], unique=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61bb6099",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 50000 documents\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"/home/student/data/camera_event_historic.csv\")\n",
    "\n",
    "records = df.to_dict(orient='records')\n",
    "\n",
    "if records:\n",
    "    result = historic_violations.insert_many(records)\n",
    "    print(f\"Inserted {len(result.inserted_ids)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3373f2",
   "metadata": {},
   "source": [
    "#### Historic Violation Collection Schema and Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c86386ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#       \"_id\": { \"$oid\": \"650e8500e29b41d4a7164467\" },\n",
    "#       \"violation_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n",
    "#       \"car_plate\": \"CJW924\",\n",
    "#       \"camera_id_start\": 1,\n",
    "#       \"camera_id_end\": 2,\n",
    "#       \"timestamp_start\": { \"$date\": \"2025-05-28T09:15:30Z\" },\n",
    "#       \"timestamp_end\":   { \"$date\": \"2025-05-28T09:16:00Z\" },\n",
    "#       \"speed_reading\": 102.5\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad85d7e",
   "metadata": {},
   "source": [
    "#### Historic Violation Collection Schema and Sample Document Data Type\n",
    "\n",
    "violation_id: string stable key\n",
    "\n",
    "car_plate: string unique business key, \n",
    "\n",
    "camera_id_start/end: ints\n",
    "\n",
    "timestamp_start/end: BSON Date\n",
    "\n",
    "speed_reading: double or null\n",
    "\n",
    "reading_type: enum \"instantaneous\"/\"average\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db6a6028",
   "metadata": {},
   "source": [
    "### Violation\n",
    "\n",
    "This collection captures every current flagged violation event. It links vehicle plates, camera pairs, timestamps, and measured speeds for real-time monitoring and windowed average‐speed calculations.\n",
    "\n",
    "The index created on the composite primary key ({car_plate, date}) in the _id field is specifically designed for efficient, practical operations central to traffic violation management systems. Practically, traffic enforcement authorities commonly require rapid access to all violation records associated with a particular vehicle on a given day, either for immediate roadside enforcement, real-time monitoring dashboards, or daily summary reports. This composite index directly supports such queries, significantly reducing retrieval times and enhancing user-facing application responsiveness by enabling a single-document lookup.\n",
    "\n",
    "Additionally, this index supports highly efficient upsert operations, essential for processing streaming violation data. When a new violation event is received, the system can immediately perform an atomic update on the specific vehicle-day document, leveraging MongoDB’s capability to quickly locate and modify the correct document using this indexed key."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a073419",
   "metadata": {},
   "source": [
    "#### Violation Collection Schema and Sample Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "31771771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# {\n",
    "#   \"_id\": {\n",
    "#     \"car_plate\": \"CJW924\",\n",
    "#     \"date\": \"2025-05-28\"\n",
    "#   },\n",
    "#   \"records\": [\n",
    "#     {\n",
    "#       \"violation_id\": \"550e8400-e29b-41d4-a716-446655440000\",\n",
    "#       \"camera_id_start\": 1,\n",
    "#       \"camera_id_end\": 2,\n",
    "#       \"timestamp_start\": { \"$date\": \"2025-05-28T09:15:30Z\" },\n",
    "#       \"timestamp_end\":   { \"$date\": \"2025-05-28T09:16:00Z\" },\n",
    "#       \"speed_reading\": 102.5,\n",
    "#       \"reading_type\": \"average\",\n",
    "#       \"processed_time\": { \"$date\": \"2025-05-28T09:16:05Z\" }\n",
    "#     }\n",
    "#   ]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5d0652",
   "metadata": {},
   "source": [
    "#### Violation Collection Schema and Sample Document Data Types\n",
    "\n",
    "_id: composite key { car_plate, date } groups all that vehicle’s daily records\n",
    "\n",
    "records: array of subdocuments (one per violation)\n",
    "\n",
    "violation_id: UUID-v5 stable key\n",
    "\n",
    "camera_id_start/end: ints\n",
    "\n",
    "timestamp_start/end, processed_time: BSON Date\n",
    "\n",
    "speed_reading: double or null\n",
    "\n",
    "reading_type: enum \"instantaneous\"/\"average\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c3e13",
   "metadata": {},
   "source": [
    "#### Violation Collection Shard Key Strategy\n",
    "\n",
    "We are deliberately choosing not to shard the Violation collection initially, based on careful consideration of expected data volumes and query patterns. The primary theoretical justification for avoiding early sharding lies in the combination of a TTL-driven retention policy and efficient indexing strategies, both of which naturally limit the working set size. Specifically, the TTL index on records.timestamp_start continuously prunes the dataset, ensuring that only the most recent 10 minutes of violation data remain active. This small, bounded working set significantly reduces the data volume that must be managed, indexed, and queried at any given time, minimizing resource demands and removing the immediate need for horizontal scaling through sharding.\n",
    "\n",
    "Additionally, the composite primary index ({_id: {car_plate, date}}) already provides efficient, targeted data access patterns, which comfortably supports expected throughput and query performance on a single-node deployment. Practically, this simplifies database administration, reducing operational complexity associated with managing a distributed system and eliminating overhead caused by inter-node coordination."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864db447",
   "metadata": {},
   "source": [
    "### Data Retention Policy\n",
    "A TTL (Time-to-Live) index on records.timestamp_start is strategically implemented to enforce a controlled data retention policy, maintaining only a 10-minute rolling window of violation data. The theoretical justification for employing such a policy originates from fundamental concepts in stream processing and real-time data analytics, where the concept of a watermark plays a critical role. A watermark explicitly defines the temporal boundaries of data that should be considered relevant for real-time computations and analytics, ensuring that computations reflect only the most recent data and minimizing potential inaccuracies due to outdated events.\n",
    "\n",
    "In stream-processing frameworks, it is essential to limit the temporal scope of data under consideration to ensure both computational efficiency and analytical accuracy. Practically, this corresponds directly with scenarios such as traffic monitoring systems, where average-speed calculations and immediate traffic alerts are valid and useful only within a recent, short-term window—typically a few minutes at most. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9db5c3",
   "metadata": {},
   "source": [
    "### Task 1.2 Collection Relationship\n",
    "\n",
    "In this task, specify the relationships between collections and explain whether you choose to embed data or store references. Justify your choice in terms of:\n",
    "* Read/write patterns\n",
    "* Data duplication versus Join cost\n",
    "* Consistency requirements (if applicable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b86d17",
   "metadata": {},
   "source": [
    "The database model clearly specifies the relationships between the Vehicle, Camera, and Violation collections by employing a strategic combination of referencing and embedding, guided by theoretical principles of normalization and denormalization, read/write performance considerations, data redundancy, and consistency requirements specific to real-time streaming scenarios.\n",
    "\n",
    "Firstly, the relationship between the Vehicle and Violation collections employs a referencing approach, where the car_plate serves as a reference from violations to vehicle metadata. This aligns closely with the principle of normalization typically applied in relational (SQL) databases, which advocates separating logically independent data entities into distinct tables to minimize redundancy, promote data integrity, and ensure ACID compliance, particularly consistency. Since vehicle information such as owner name, address, and vehicle type is relatively static and rarely updated, embedding this metadata directly into every violation document would create substantial data duplication. Such redundancy could drastically increase storage overhead and computational costs associated with updates. For instance, if an owner’s address changes, it would necessitate updating potentially thousands of violation documents, posing significant performance penalties and increasing risk of data inconsistency. By referencing, updates remain centralized within the Vehicle collection, ensuring strong consistency by providing immediate reflection of any metadata changes across all referencing violation records. Although referencing requires performing additional join operations, these lookups are minimal, involving single-document reads, and the join cost remains acceptably low compared to the heavy costs associated with data duplication and mass updates.\n",
    "\n",
    "Similarly, the relationship between the Camera and Violation collections also uses a referencing strategy, employing the camera_id as a foreign key. Cameras represent static, rarely changing metadata, including latitude, longitude, position, and speed limits. Embedding these static details into every violation record (denormalization) would result in significant duplication, increasing storage consumption and complicating updates whenever camera parameters change, like a speed limit adjustment or positional shift. Considering that violation events occur frequently and can number into millions daily, embedding would escalate storage costs substantially. The referencing approach, inspired by relational database normalization principles, mitigates these issues by centralizing camera metadata updates within the Camera collection. Although referencing incurs a small cost through joining camera data during violation processing, these joins are efficient due to indexed camera identifiers, resulting in minimal computational overhead. Therefore, the referencing choice provides a practical balance between storage efficiency, computational overhead, and data consistency, reflecting best practices commonly observed in real-world traffic monitoring systems.\n",
    "\n",
    "Conversely, within the Violation collection itself, the relationship among individual violation records adopts an embedding strategy through denormalization. Specifically, violation records are embedded directly into parent documents grouped by a composite key (car_plate and date). This embedding approach aligns closely with denormalization principles typical of NoSQL databases like MongoDB, where frequently accessed and logically related data is combined into single documents to enable fast query responses. Embedding violation records greatly enhances read performance which is essential in real-time streaming applications where queries frequently request “all violations for a particular vehicle on a specific day.” While this strategy introduces controlled data duplication, such as repeated storage of car_plate within embedded records, this redundancy is minimal and justified by significant performance improvements during queries, such as generating real-time dashboards, immediate alerts, or daily summary reports. Furthermore, embedding leverages MongoDB's atomic update operations, allowing safe, concurrent, idempotent inserts of violation records without requiring costly multi-document transactions.\n",
    "\n",
    "Finally, given that the database serves a real-time streaming application focused on traffic violation enforcement, data consistency emerges as a critical consideration. In this scenario, strong consistency is favored over eventual consistency due to the necessity of providing enforcement authorities with immediate and accurate violation information. When traffic officers query the system, it is essential that the latest violation data be returned immediately to support timely enforcement decisions and accurate legal documentation. Eventual consistency, characterized by delayed updates and stale reads, would be inappropriate here, as even brief delays could undermine real-time enforcement activities. Thus, by utilizing references for static data and embedding for frequently accessed data, this design inherently supports strong consistency. Centralized, authoritative collections (Vehicle and Camera) ensure consistent reference lookups, while atomic embedded updates in Violations ensure immediate, accurate, and reliable data states.\n",
    "\n",
    "In summary, the database design strategically integrates referencing and embedding, carefully balancing normalization's benefits (consistency, reduced redundancy, centralized updates) against denormalization's performance advantages (fast query processing, efficient reads). This approach ensures robust performance and strong data consistency suitable for real-time streaming enforcement scenarios, directly aligned with theoretical database design principles and real-world practical requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e797ff17",
   "metadata": {},
   "source": [
    "### Task 1.3 Discussion\n",
    "\n",
    "In this task, discuss whether your model supports\n",
    "* Consistency and Idempotency\n",
    "    * Does it support idempotent writes?\n",
    "    * Explain any upsert pattern in `violation` collection\n",
    "* Scalability and Fault-Tolerance\n",
    "    * Can your data model support high ingest rates?\n",
    "    * Can your data model support low-latency lookups?\n",
    "\n",
    "Justify and explain the trade-off made in your design."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22df226",
   "metadata": {},
   "source": [
    "Firstly, the designed data model strongly supports both consistency and idempotency, especially through the explicit use of an upsert pattern in the Violation collection. Idempotency, a key theoretical principle in distributed systems, ensures repeated identical writes yield the same state as a single write. This model achieves idempotency by employing a stable, deterministic identifier for each violation record, generated using UUID version 5 (uuid5) based on a carefully chosen composite key (vehicle's car_plate, camera_id_start, camera_id_end, and violation's timestamp_start). Since UUIDv5 is generated deterministically from its inputs, repeated ingestion attempts of the same violation data result in the same UUID, ensuring no duplicate records are ever created, regardless of how many times the same event is processed. This approach directly addresses real-world scenarios common in streaming pipelines, where message retries or duplicated data batches frequently occur.\n",
    "\n",
    "The consistency of the data model is further reinforced by the chosen upsert strategy. Each violation is appended to an array embedded within a parent document keyed by vehicle plate and date. MongoDB's atomic updateOne with $addToSet guarantees that each violation sub-document is inserted exactly once, or not at all, preventing partial writes and preserving document-level atomicity. This single-document operation avoids complexities typically associated with multi-document transactions, adhering closely to MongoDB's strengths. In practical traffic violation systems, immediate consistency at the violation level is critical, particularly when generating accurate real-time alerts or official records; this approach precisely meets these requirements.\n",
    "\n",
    "Secondly, the database design strongly supports scalability and fault-tolerance, effectively handling high ingest rates and low-latency lookups. The Violation collection is particularly designed to handle significant ingestion volume through its carefully structured document model. By grouping violation events per vehicle per day, the database can handle a high rate of incoming violation records efficiently, reducing lock contention and enabling high-throughput inserts. The MongoDB single-document append operations ($addToSet) on the embedded arrays scale effectively, as updates are targeted at specific vehicle-day combinations. Theoretically, this arrangement minimizes write contention, which is essential for scalability, especially in a high-velocity traffic monitoring scenario that involves thousands or millions of daily violation events.\n",
    "\n",
    "Additionally, low-latency lookups are strongly supported through the careful indexing strategy. Each Violation document's composite key ({ car_plate, date }) provides rapid, constant-time lookups for typical queries (e.g., \"all violations for a particular vehicle today\"). The minimal join pattern, which is due to the referencing of static camera and vehicle metadata, ensures fast retrieval performance, critical for real-time dashboards and analytical queries commonly used by enforcement agencies. In real-world terms, a traffic-monitoring or enforcement system demands quick, efficient lookups. For instance, when law enforcement agents need immediate violation histories on the road, thus making this design particularly suitable.\n",
    "\n",
    "Regarding fault-tolerance and resilience, the model effectively balances data consistency and performance through built-in MongoDB features such as Time-to-Live (TTL) indexes. The TTL index on \"records.timestamp_start\" enforces automatic data retention policies, continuously purging data older than the specified retention window (e.g., 10 minutes), ensuring storage use remains predictable and the data working set size stays manageable. This approach safeguards the database against uncontrolled growth and guarantees predictable performance, especially during real-time windowed computations, such as calculating average speeds over specific intervals. In real-world deployments, such design choices significantly reduce operational complexity and enhance system stability.\n",
    "\n",
    "In conclusion, this data model consciously balances consistency, idempotency, scalability, and fault tolerance, carefully addressing practical requirements and theoretical considerations simultaneously. The deterministic UUID-based idempotency, atomic MongoDB upsert operations, strategic indexing, and automatic data expiration policy collectively support robust real-world traffic monitoring and enforcement scenarios. Trade-offs are explicitly made, like favoring minimal referencing over embedding for static data, to achieve optimal performance, storage efficiency, and simplicity. This ensures the system not only handles high ingestion rates and real-time lookups effectively but also maintains consistent, accurate data under realistic operational conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eafe9a9",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 2: Streaming Application</span>\n",
    "\n",
    "This section consists of 2 sub-questions. \n",
    "\n",
    "In this task, you will implement a streaming application to simulate the AWAS system. Figure 2 illustrates an overview of the streaming architecture that is to be developed to simulate AWAS. Implementation is expected to be following programming standards with high readability (supported with documentation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b51a153",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\"> \n",
    "    <img src=\"FIT3182_A2_Fig_2.png\"></img>\n",
    "    <p style=\"text-align: center\">Figure 2 - Overview of streaming application to simulate AWAS </p>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474fc3a8",
   "metadata": {},
   "source": [
    "### Task 2.1 Data Stream Processing\n",
    "\n",
    "In this task, you will implement multiple **Apache Kafka** producers to simulate the real-time streaming of the data, which will be processed by **Apache Spark Structured Streaming** client and then inserted into MongoDB.\n",
    "\n",
    "*<span style=\"color:red\">Important note:</span> You are expected to use the same data model from Task 1. To make the streaming data consistent for the model, you may need to make some changes to the streaming data before building the model or inserting it to MongoDB.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a989065f",
   "metadata": {},
   "source": [
    "#### Event Producer\n",
    "\n",
    "**Event Producer A**: Write a python program that loads all the data from `camera_event_A.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_a.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer B**: Write a python program that loads all the data from `camera_event_B.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_b.ipynb**, where **xxx** represents the student IDs of the group members.\n",
    "\n",
    "**Event Producer C**: Write a python program that loads all the data from `camera_event_C.csv` and feed the data to the stream in batches every $n$ seconds. You can refer to the batch id column in the csv file to identify the data to read and send to the stream. The typical value for $n$ is 5 seconds. You might need to append additional information such as producer information to identify the producer. If you are running this producer in a separate Jupyter notebook file, save the file as **xxx_assignment02_producer_c.ipynb**, where **xxx** represents the student IDs of the group members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0c7a85d",
   "metadata": {},
   "source": [
    "#### Streaming Application\n",
    "\n",
    "Write a streaming application using Apache Spark Structured Streaming API which processes data in batches. Each batch should contain 0 or more camera event (from event producer). The streaming application should process the data as follows.\n",
    "* Join the streaming data from the producers and determine if a vehicle should be flagged as violation. You should drop any data pair if the timestamp and the order of the camera does not match.\n",
    "* If there is a violation detected, store it into MongoDB straight away.\n",
    "* If there is no violation detected, drop the record.\n",
    "* Due to the dynamic nature of moving vehicle, the time of vehicle completing the camera segment may vary and you should decide how many records and how long the records should be stored in the buffer until a pair is identified.\n",
    "\n",
    "##### Violation Detection Rule\n",
    "A vehicle is flagged as violating the speed limit if any one of the following happens.\n",
    "* Instantaneous speed of vehicle exceed the speed limit of the recording camera\n",
    "* Average speed of vehicle exceed the speed limit of the ending camera\n",
    "\n",
    "<span style=\"color:red\">Important Note:</span> *Only one record for a car per day is recorded in the database. If the car violates at **different cameras**, the record should be merged together into a single record to be stored in the database.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fcebac59",
   "metadata": {},
   "outputs": [],
   "source": [
    "host_ip = \"10.192.64.67\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9747578c",
   "metadata": {},
   "source": [
    "This DbWriter class defines a custom writer for Spark Structured Streaming to persist processed violation data into a MongoDB collection. For each micro-batch partition, the open() method establishes a MongoDB connection. The process() method generates a stable violation_id using a UUID based on unique vehicle and event data, constructs a structured record containing violation details (e.g., timestamps, speed, type), and performs an upsert into the violations collection. Each record is nested inside a parent document identified by a composite key of {car_plate, date}. The $addToSet operator ensures records are not duplicated. Finally, the close() method safely closes the MongoDB connection after all rows are processed. This approach ensures idempotent writes and maintains clean, daily-grouped violation logs per vehicle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "daf4db04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DbWriter:\n",
    "    def open(self, partition_id, epoch_id):\n",
    "        self.mongo_client = MongoClient(\n",
    "            host=f'{host_ip}',\n",
    "            port=27017\n",
    "        )\n",
    "        self.db = self.mongo_client['fit3182_db']\n",
    "        return True\n",
    "\n",
    "    def process(self, row):\n",
    "        try:\n",
    "            key_string = (\n",
    "                f\"{row.car_plate}-\"\n",
    "                f\"{row.camera_id_start}-\"\n",
    "                f\"{row.camera_id_end}-\"\n",
    "                f\"{int(row.timestamp_start.timestamp())}\"\n",
    "            )\n",
    "            \n",
    "            stable_id = str(uuid.uuid5(uuid.NAMESPACE_OID, key_string))\n",
    "            record = {\n",
    "                \"violation_id\":   stable_id,\n",
    "                \"camera_id_start\": row.camera_id_start,\n",
    "                \"camera_id_end\":   row.camera_id_end,\n",
    "                \"timestamp_start\": row.timestamp_start.replace(tzinfo=None),\n",
    "                \"timestamp_end\":   row.timestamp_end.replace(tzinfo=None),\n",
    "                \"speed_reading\":   float(row.speed_reading)\n",
    "                                    if row.speed_reading is not None else None,\n",
    "                \"reading_type\":   (\n",
    "                    \"instantaneous\"\n",
    "                    if row.camera_id_start == row.camera_id_end\n",
    "                    else \"average\"\n",
    "                ),\n",
    "                \"processed_time\": row.processed_time.replace(tzinfo=None)\n",
    "            }\n",
    "\n",
    "            doc_id = {\n",
    "                \"car_plate\": row.car_plate,\n",
    "                \"date\":      row.timestamp_start.date().isoformat()\n",
    "            }\n",
    "            \n",
    "            collection = self.db.violations\n",
    "\n",
    "            collection.update_one(\n",
    "                {\"_id\": doc_id},\n",
    "                {\"$addToSet\": {\"records\": record}},          \n",
    "                upsert=True\n",
    "            )\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"DbWriter error:\", e)\n",
    "    \n",
    "    def close(self, err):\n",
    "        self.mongo_client.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6387dc15",
   "metadata": {},
   "source": [
    "The codes creates a pyspark instance and connects it to the kafka streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "04e9bda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-streaming-kafka-0-10_2.12:3.3.0,org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "98e4ed4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .master('local[*]')\n",
    "    .appName('Streaming AWAS Data')\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78513f1b",
   "metadata": {},
   "source": [
    "The Code belows defines the 3 producer, and uses Kafka to subscribe to the 3 produce to simulate the streams of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b9b9fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = ['cameraA','cameraB','cameraC']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "969fdc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kafka_sdf = (\n",
    "    spark.readStream\n",
    "    .format('kafka')\n",
    "    .option('kafka.bootstrap.servers', f'{host_ip}:9092')\n",
    "    .option('subscribe', ','.join(topics))\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f90f113e",
   "metadata": {},
   "source": [
    "The code belows uses a Spark Processing and collects them using query into a live updating dataframe with the topic (camera) and the value which are the values of records (data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78779dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_df = kafka_sdf.selectExpr(\n",
    "    \"CAST(topic AS STRING)   AS topic\",\n",
    "    \"CAST(value AS STRING)   AS value\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f0cadcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: string (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stream_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d472df",
   "metadata": {},
   "source": [
    "Below is a json schema which the data are stored in the format after retrieving from the streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c36cfce",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_schema = StructType([\n",
    "    StructField(\"event_id\",     StringType(),  True),\n",
    "    StructField(\"batch_id\",     IntegerType(), True),\n",
    "    StructField(\"car_plate\",    StringType(),  True),\n",
    "    StructField(\"camera_id\",    IntegerType(), True),\n",
    "    StructField(\"timestamp\",    StringType(),  True),\n",
    "    StructField(\"speed_reading\",DoubleType(),  True),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7366fb2",
   "metadata": {},
   "source": [
    "This code is part of a Spark Structured Streaming pipeline that processes incoming Kafka messages. It begins by parsing the JSON-formatted value field from the Kafka stream using a predefined schema, extracting structured data into a new data column. From this parsed column, specific fields such as event_id, batch_id, car_plate, camera_id, timestamp, and speed_reading are selected and aliased for clarity. The timestamp field is converted into a proper Spark TimestampType using to_timestamp, enabling accurate time-based operations. A critical feature in this step is the use of a 10-minute watermark on the timestamp column, which instructs Spark to manage state and late-arriving data efficiently. The watermark ensures that records older than 10 minutes from the latest event are not considered in aggregations, thereby controlling memory usage and preventing stale data from affecting results. This transformation prepares the stream for downstream analytics by ensuring both schema consistency and time-awareness, making it suitable for real-time use cases such as violation detection and speed averaging.\n",
    "\n",
    "In our scenario, any data arriving more than 10 minutes late is treated as invalid and not considered a speed violation. The watermark mechanism ensures that moderately delayed records—those within the 10-minute threshold—are still processed accurately. This approach balances tolerance for network or processing latency with the need to discard excessively late data that could distort real-time violation analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b892be88",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = stream_df \\\n",
    "    .withColumn(\"data\", from_json(col(\"value\"), json_schema)) \\\n",
    "    .select(\n",
    "        col(\"topic\"),\n",
    "        col(\"data.event_id\").alias(\"event_id\"),\n",
    "        col(\"data.batch_id\").alias(\"batch_id\"),\n",
    "        col(\"data.car_plate\").alias(\"car_plate\"),\n",
    "        col(\"data.camera_id\").alias(\"camera_id\"),\n",
    "        to_timestamp(col(\"data.timestamp\")).alias(\"timestamp\"),\n",
    "        col(\"data.speed_reading\").alias(\"speed_reading\")\n",
    "    ).withWatermark(\"timestamp\", \"10 minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1b8920ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- topic: string (nullable = true)\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- batch_id: integer (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- camera_id: integer (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "parsed_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70df2b96",
   "metadata": {},
   "source": [
    "Data from the streams are filtered out according to their topic (camera) for easier processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dbc4c64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cameraA_df = parsed_df.filter(col(\"topic\") == \"cameraA\")\n",
    "cameraB_df = parsed_df.filter(col(\"topic\") == \"cameraB\")\n",
    "cameraC_df = parsed_df.filter(col(\"topic\") == \"cameraC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cc7433",
   "metadata": {},
   "source": [
    "The violations_df DataFrame instantly flags vehicles as violators if their recorded instantaneous speed exceeds the predefined speed limit set for the corresponding camera location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6f83ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_limits = parsed_df.withColumn(\n",
    "    \"speed_limit\",\n",
    "    when(col(\"camera_id\") == 1, 110.0)\n",
    "    .when(col(\"camera_id\") == 2, 110.0)\n",
    "    .when(col(\"camera_id\") == 3, 90.0)\n",
    "    .otherwise(90.0) \n",
    ")\n",
    "\n",
    "violations_df = (\n",
    "    with_limits\n",
    "      .filter(col(\"speed_reading\") > col(\"speed_limit\"))\n",
    "      .select(\n",
    "          col(\"event_id\"),\n",
    "          col(\"car_plate\"),\n",
    "          col(\"camera_id\").alias(\"camera_id_start\"),\n",
    "          col(\"camera_id\").alias(\"camera_id_end\"),\n",
    "          col(\"timestamp\").alias(\"timestamp_start\"),\n",
    "          col(\"timestamp\").alias(\"timestamp_end\"),\n",
    "          col(\"speed_reading\"),\n",
    "          col(\"speed_limit\")\n",
    "      )\n",
    "    .withColumn(\"processed_time\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00c75e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- event_id: string (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- camera_id_start: integer (nullable = true)\n",
      " |-- camera_id_end: integer (nullable = true)\n",
      " |-- timestamp_start: timestamp (nullable = true)\n",
      " |-- timestamp_end: timestamp (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      " |-- speed_limit: double (nullable = false)\n",
      " |-- processed_time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "violations_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc2a09",
   "metadata": {},
   "source": [
    "This PySpark code block creates a transformed DataFrame called joined_AB that identifies average speed violations by joining two streaming DataFrames: cameraA_df and cameraB_df, each representing traffic events captured by different cameras. The join is performed based on matching car plate numbers and a temporal condition where the timestamp from camera B occurs within 33 seconds after the timestamp from camera A. This ensures that the two events likely correspond to the same vehicle moving between two checkpoints within a reasonable time window. The selected columns include identifiers, timestamps, and speed readings from both cameras, along with the calculated travel time in seconds (travelTimeSec). Using this, the code computes the average speed over the segment using the formula 3600 / travelTimeSec, converting time into a speed measurement in km/h. If the travel time is invalid (e.g., zero or negative), the speed is set to None for safety. Finally, the processed_time column captures the current system timestamp when the row is processed, which can be useful for monitoring delays or applying watermarking in further streaming logic. Overall, this transformation is designed to facilitate the detection and analysis of average speed violations in near real-time.\n",
    "\n",
    "The interval of 33 seconds is calculated by:\n",
    "\n",
    "Speed = (Distance / Time) \n",
    "\n",
    "where in our case:\n",
    "1. Distance = 1 KM\n",
    "2. Speed (The predefined Speed Limit which is the highest speed that a person can drive without violating the law) = 110KM/H & 90 KM/H\n",
    "\n",
    "Hence,\n",
    "Fastest Time taken without being a violator = (1/110)*3600 & (1/90)*3600 which are 33s and 40s.\n",
    "\n",
    "\n",
    "Note that,\n",
    "\n",
    "joined_AB -> joins between Cam A and Cam B with speed limit of 110km/h\n",
    "\n",
    "joined_BC -> joins between Cam B and Cam C with speed limit of 90 km/h\n",
    "\n",
    "joined_AB_safe -> are the dropped pairs (which are logged in the console) for the non violators between Cam A and Cam B.\n",
    "\n",
    "joined_BC_safe -> are the dropped pairs (which are logged in the console) for the non violators between Cam B and Cam C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df493254",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_AB = (\n",
    "    cameraA_df.alias(\"a\")\n",
    "      .join(\n",
    "          cameraB_df.alias(\"b\"),\n",
    "          expr(\"\"\"\n",
    "            a.car_plate = b.car_plate\n",
    "            AND b.timestamp BETWEEN \n",
    "                a.timestamp\n",
    "            AND a.timestamp + INTERVAL 33 SECONDS\n",
    "          \"\"\"),\n",
    "          how=\"inner\"\n",
    "      )\n",
    "      .select(\n",
    "        col(\"a.event_id\").alias(\"eventA\"),\n",
    "        col(\"b.event_id\").alias(\"eventB\"),\n",
    "        col(\"a.car_plate\"),\n",
    "        col(\"a.camera_id\").alias(\"camera_id_start\"),\n",
    "        col(\"b.camera_id\").alias(\"camera_id_end\"),\n",
    "        col(\"a.timestamp\").alias(\"timestamp_start\"),\n",
    "        col(\"b.timestamp\").alias(\"timestamp_end\"),\n",
    "        col(\"a.speed_reading\").alias(\"speedA\"),\n",
    "        col(\"b.speed_reading\").alias(\"speedB\"),\n",
    "        (col(\"b.timestamp\").cast(\"long\") - col(\"a.timestamp\").cast(\"long\"))\n",
    "            .alias(\"travelTimeSec\")\n",
    "    )\n",
    "    .withColumn(\"speed_reading\",\n",
    "          when(col(\"travelTimeSec\") > 0,\n",
    "               expr(\"CAST(3600.0 / travelTimeSec AS double)\"))\n",
    "          .otherwise(None)                 # or filter/throw, up to you\n",
    "      )\n",
    "    .withColumn(\"processed_time\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e8be1f03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventA: string (nullable = true)\n",
      " |-- eventB: string (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- camera_id_start: integer (nullable = true)\n",
      " |-- camera_id_end: integer (nullable = true)\n",
      " |-- timestamp_start: timestamp (nullable = true)\n",
      " |-- timestamp_end: timestamp (nullable = true)\n",
      " |-- speedA: double (nullable = true)\n",
      " |-- speedB: double (nullable = true)\n",
      " |-- travelTimeSec: long (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      " |-- processed_time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_AB.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2c8c9527",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_AB_safe = (\n",
    "    cameraA_df.alias(\"a\")\n",
    "      .join(\n",
    "          cameraB_df.alias(\"b\"),\n",
    "          expr(\"\"\"\n",
    "            a.car_plate = b.car_plate\n",
    "            AND b.timestamp > a.timestamp + INTERVAL 33 SECONDS\n",
    "          \"\"\"),\n",
    "          how=\"inner\"\n",
    "      )\n",
    "      .select(\n",
    "        col(\"a.event_id\").alias(\"eventA\"),\n",
    "        col(\"b.event_id\").alias(\"eventB\"),\n",
    "        col(\"a.car_plate\"),\n",
    "        col(\"a.camera_id\").alias(\"camera_id_start\"),\n",
    "        col(\"b.camera_id\").alias(\"camera_id_end\"),\n",
    "        col(\"a.timestamp\").alias(\"timestamp_start\"),\n",
    "        col(\"b.timestamp\").alias(\"timestamp_end\"),\n",
    "        col(\"a.speed_reading\").alias(\"speedA\"),\n",
    "        col(\"b.speed_reading\").alias(\"speedB\"),\n",
    "        (col(\"b.timestamp\").cast(\"long\") - col(\"a.timestamp\").cast(\"long\")).alias(\"travelTimeSec\")\n",
    "      )\n",
    "      .withColumn(\"average_speed\",\n",
    "          when((col(\"travelTimeSec\") > 0) & col(\"travelTimeSec\").isNotNull(),\n",
    "               expr(\"CAST(3600.0 / travelTimeSec AS double)\"))\n",
    "          .otherwise(None)\n",
    "      )\n",
    "      .withColumn(\"processed_time\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4d80e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_BC = (\n",
    "    cameraB_df.alias(\"b\")\n",
    "      .join(\n",
    "          cameraC_df.alias(\"c\"),\n",
    "          expr(\"\"\"\n",
    "            b.car_plate = c.car_plate\n",
    "            AND c.timestamp BETWEEN \n",
    "                b.timestamp\n",
    "            AND b.timestamp + INTERVAL 40 SECONDS\n",
    "          \"\"\"),\n",
    "          how=\"inner\"\n",
    "      )\n",
    "      .select(\n",
    "        col(\"b.event_id\").alias(\"eventB\"),\n",
    "        col(\"c.event_id\").alias(\"eventC\"),\n",
    "        col(\"b.car_plate\"),\n",
    "        col(\"b.camera_id\").alias(\"camera_id_start\"),\n",
    "        col(\"c.camera_id\").alias(\"camera_id_end\"),\n",
    "        col(\"b.timestamp\").alias(\"timestamp_start\"),\n",
    "        col(\"c.timestamp\").alias(\"timestamp_end\"),\n",
    "        col(\"b.speed_reading\").alias(\"speedB\"),\n",
    "        col(\"c.speed_reading\").alias(\"speedC\"),\n",
    "        (col(\"c.timestamp\").cast(\"long\") - col(\"b.timestamp\").cast(\"long\"))\n",
    "            .alias(\"travelTimeSec\")\n",
    "    )\n",
    "    .withColumn(\"speed_reading\",\n",
    "          when(col(\"travelTimeSec\") > 0,\n",
    "               expr(\"CAST(3600.0 / travelTimeSec AS double)\"))\n",
    "          .otherwise(None) \n",
    "    )\n",
    "    .withColumn(\"processed_time\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2936ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- eventB: string (nullable = true)\n",
      " |-- eventC: string (nullable = true)\n",
      " |-- car_plate: string (nullable = true)\n",
      " |-- camera_id_start: integer (nullable = true)\n",
      " |-- camera_id_end: integer (nullable = true)\n",
      " |-- timestamp_start: timestamp (nullable = true)\n",
      " |-- timestamp_end: timestamp (nullable = true)\n",
      " |-- speedB: double (nullable = true)\n",
      " |-- speedC: double (nullable = true)\n",
      " |-- travelTimeSec: long (nullable = true)\n",
      " |-- speed_reading: double (nullable = true)\n",
      " |-- processed_time: timestamp (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_BC.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "67958aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_BC_safe = (\n",
    "    cameraB_df.alias(\"b\")\n",
    "      .join(\n",
    "          cameraC_df.alias(\"c\"),\n",
    "          expr(\"\"\"\n",
    "            b.car_plate = c.car_plate\n",
    "            AND c.timestamp > b.timestamp + INTERVAL 40 SECONDS\n",
    "          \"\"\"),\n",
    "          how=\"inner\"\n",
    "      )\n",
    "      .select(\n",
    "        col(\"b.event_id\").alias(\"eventB\"),\n",
    "        col(\"c.event_id\").alias(\"eventC\"),\n",
    "        col(\"b.car_plate\"),\n",
    "        col(\"b.camera_id\").alias(\"camera_id_start\"),\n",
    "        col(\"c.camera_id\").alias(\"camera_id_end\"),\n",
    "        col(\"b.timestamp\").alias(\"timestamp_start\"),\n",
    "        col(\"c.timestamp\").alias(\"timestamp_end\"),\n",
    "        col(\"b.speed_reading\").alias(\"speedB\"),\n",
    "        col(\"c.speed_reading\").alias(\"speedC\"),\n",
    "        (col(\"c.timestamp\").cast(\"long\") - col(\"b.timestamp\").cast(\"long\"))\n",
    "            .alias(\"travelTimeSec\")\n",
    "    )\n",
    "    .withColumn(\"speed_reading\",\n",
    "          when((col(\"travelTimeSec\") > 0) & col(\"travelTimeSec\").isNotNull(),\n",
    "               expr(\"CAST(3600.0 / travelTimeSec AS double)\"))\n",
    "          .otherwise(None)\n",
    "    )\n",
    "    .withColumn(\"processed_time\", current_timestamp())\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9721920a",
   "metadata": {},
   "source": [
    "This section of the code defines multiple streaming queries that handle different types of traffic violation data as it flows through a Spark Structured Streaming pipeline. Each query processes a specific DataFrame, either by writing the results to the console for real-time monitoring or persisting them to MongoDB using a custom DbWriter class.\n",
    "\n",
    "The ABquery and BCquery queries are responsible for writing average speed violations between camera A to B and B to C, respectively, into the MongoDB violations collection. They use foreach(DbWriter()) to handle the writing logic row by row, ensuring each violation record is stored efficiently with structured identifiers and metadata like processing time.\n",
    "\n",
    "In contrast, the ABsafequery and BCsafequery queries serve a debugging or monitoring purpose. Instead of saving to a database, they simply write the processed stream from joined_AB_safe and joined_BC_safe to the console. These \"safe\" joins often include stricter conditions (e.g., extended intervals or cleansed data), providing a clearer view of stream accuracy and filtering performance.\n",
    "\n",
    "Lastly, the violation_query stream handles violations_df, which likely includes instantaneous speed violations, such as a single camera detecting a speed over the legal limit. These are also written to MongoDB using the same DbWriter, allowing comprehensive storage of both average and instantaneous violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2b6e267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABquery = (\n",
    "    joined_AB\n",
    "      .writeStream\n",
    "      .outputMode(\"append\")\n",
    "      .foreach(DbWriter())\n",
    "      .option(\"truncate\", \"False\")\n",
    "      .start()\n",
    ")\n",
    "\n",
    "ABsafequery = (\n",
    "    joined_AB_safe\n",
    "      .writeStream\n",
    "      .outputMode(\"append\")\n",
    "      .format(\"console\")\n",
    "      .option(\"truncate\", \"False\")\n",
    "      .start()\n",
    ")\n",
    "\n",
    "BCquery = (\n",
    "    joined_BC\n",
    "      .writeStream\n",
    "      .outputMode(\"append\")\n",
    "      .foreach(DbWriter())\n",
    "      .option(\"truncate\", \"False\")\n",
    "      .start() \n",
    ")\n",
    "\n",
    "BCsafequery = (\n",
    "    joined_BC_safe\n",
    "      .writeStream\n",
    "      .outputMode(\"append\")\n",
    "      .format(\"console\")\n",
    "      .option(\"truncate\", \"False\")\n",
    "      .start() \n",
    ")\n",
    "\n",
    "violation_query = (\n",
    "    violations_df\n",
    "      .writeStream\n",
    "      .outputMode(\"append\")\n",
    "      .foreach(DbWriter())\n",
    "      .option(\"truncate\", \"False\")\n",
    "      .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c4ab34",
   "metadata": {},
   "source": [
    "This block gracefully waits for any active streaming queries to finish. If the user interrupts with CTRL-C, it stops the Spark session and shuts down all streams cleanly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20e5c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    spark.streams.awaitAnyTermination()\n",
    "except KeyboardInterrupt:\n",
    "    print('Interrupted by CTRL-C. Stopping query.')\n",
    "finally:\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfce5de",
   "metadata": {},
   "source": [
    "### Task 2.2 Data Visualisation\n",
    "\n",
    "In this task, you will implement a program to visualize the joined streaming data. For the incoming camera event(s), \n",
    "* plot the number of violation against arrival time. You need to label some interesting points such as maximum and minimum values. \n",
    "* In addition to that, plot the speed against arrival time. You need to include some interesting points such as average and maximum values.\n",
    "\n",
    "For visualization on the data stored in the database, you have to plot a map using camera location. On the map, annotate\n",
    "* number of violations between the checkpoints\n",
    "* identify hotspot (e.g. when number of violations exceed certain threshold within a time in a day)\n",
    "\n",
    "Explain and justify the plots and the inclusion of the interesting points. Set your own threshold for the hotspot.\n",
    "\n",
    "If you are running this task in a separate Jupyter notebook file, save the file as **xxx_assignment02_visualisation.ipynb**, where **xxx** represents the student IDs of the group members."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f748b82f",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 3: Documentation and comments to describe the proposed solution in the submitted notebook</span>\n",
    "\n",
    "You should include sufficient comments and explanation Tasks 1 and 2 to describe your algorithm and/or code implementation. Please add additional markdown cells to explain your work. Adding extra illustrations to describe your method will also add to the marks in this part."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f29b622",
   "metadata": {},
   "source": [
    "## <span style=\"color:#0b486b\">Part 4: Code demo and interview</span>\n",
    "\n",
    "In this task, you will present and showcase the simulation. After the assignment due date, you will be asked to attend an interview/demo session to showcase your application. Your interviewer will ask you a few questions in relation to your application and assess your understanding.\n",
    "\n",
    "During the code demo, your work will be evaluated and assessed based on the marking guideline. Group members will obtain the same marks based on the code demo, unless there is an imbalance in contributions between students in a team. Additionally, each team member will be interviewed to explain the submitted work. The interview represents an individual assessment and a score between 0 and 1 will be awarded, which is then multipled with the marks obtained during the code demo.\n",
    "\n",
    "Interviews for Assignment-2 will be conducted during Week 12 lab sessions. If you are granted an extension from special consideration, the interview will be conducted during SWOT-VAC week."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
